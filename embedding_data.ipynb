{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47d3c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:  ['name', 'sector', 'technologies', 'city', 'description']\n",
      "rows before cleaning:  324\n",
      "name has : 0 missing values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config import PATHS\n",
    "\n",
    "#data before cleaning: \n",
    "df = pd.read_json(PATHS['raw_data'], encoding='utf-8', orient='records')\n",
    "print(\"columns: \", df.columns.tolist())\n",
    "print(\"rows before cleaning: \", df.shape[0])\n",
    "\n",
    "df.replace(['N/A', \"\", None], 'unknown', inplace=True)\n",
    "df.fillna('unknown', inplace=True)\n",
    "\n",
    "print(f\"name has : {df[(df.name == 'N/A') | (df.name == '') | (df.name.isna())].shape[0] } missing values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded5a7f",
   "metadata": {},
   "source": [
    "We should avoid dropping any record just because one feature is missing, the only ones to drop are those with all features missing. \n",
    "The reason is that we want all startups to be included, dropping some means less couverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484f6d8",
   "metadata": {},
   "source": [
    "So there's actually no row to drop. I will see what combinations of features are missing, after that I will use present features to generate text that will be used in text embedding. (the most import feature for this is obviously 'description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9eaee68-0a1c-4cc4-9066-b69253d90259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows with ['sector'] missing\n",
      "45 rows with ['technologies'] missing\n",
      "1 rows with ['city'] missing\n",
      "6 rows with ['description'] missing\n",
      "\n",
      "\n",
      "1 rows with ['sector', 'technologies'] missing\n",
      "0 rows with ['sector', 'city'] missing\n",
      "0 rows with ['sector', 'description'] missing\n",
      "0 rows with ['technologies', 'city'] missing\n",
      "2 rows with ['technologies', 'description'] missing\n",
      "0 rows with ['city', 'description'] missing\n",
      "\n",
      "\n",
      "0 rows with ['sector', 'technologies', 'city'] missing\n",
      "0 rows with ['sector', 'technologies', 'description'] missing\n",
      "0 rows with ['sector', 'city', 'description'] missing\n",
      "0 rows with ['technologies', 'city', 'description'] missing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "features = df.columns.tolist()\n",
    "features.remove('name')  # Exclude 'name' from combinations since it's not a feature\n",
    "def missing_combinations(): \n",
    "    for p in range(1, len(features)):\n",
    "        combos = [list(uplet) for uplet in combinations(features, p)]\n",
    "        for combo in combos:\n",
    "            def all_missing(row): return all(row[col] == 'unknown' for col in combo)\n",
    "            print(df[df.apply(all_missing, axis=1)].shape[0], f\"rows with {combo} missing\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "missing_combinations()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ed87b",
   "metadata": {},
   "source": [
    "(6 rows with ['description'] missing) this this the most important thing to fix. we see from results above that Alhamdulillah when 'description' is missing, ALL the other features are present 92% of the time, and MOST of them are present 100% of the time. So I will try to generate a description based on present features when it's missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675a422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data for text embedding\n",
    "import re\n",
    "\n",
    "#print(embedding_df[embedding_df['description'] == 'unknown'])\n",
    "def normalize_text(text):\n",
    "    return text.strip().lower().replace(\"’\", \"'\") if isinstance(text, str) else text.str.strip().lower()\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].map(normalize_text)\n",
    "\n",
    "cleaned_names =[(name.replace(\"’\", \"'\").replace(\",\", \"\").replace(\";\", \"\").replace(\":\", \"\")\n",
    "                      .replace(\"/\", \"\").replace(\"\\\\\", \"\").replace(\"-\", \"\").replace(\".\", \"\").strip().lower()) for name in df.name]\n",
    "df.index = cleaned_names\n",
    "df.drop(columns=['name'], inplace=True)\n",
    "\n",
    "#remove startups names from description\n",
    "def remove_name_from_description(df):\n",
    "    for name in df.index:\n",
    "        description = df.loc[name, 'description']\n",
    "        for word in name.split():\n",
    "            word_pattern = rf\"\\b{re.escape(word)}\\b\"  \n",
    "            description = re.sub(word_pattern, '', description)\n",
    "\n",
    "        description = re.sub(r'\\s+', ' ', description)                     # multiple spaces to one space\n",
    "        description = re.sub(r'\\s*,\\s*', ', ', description)               # clean commas\n",
    "        description = re.sub(r',\\s*,', ',', description)                  # handle multiple commas\n",
    "        description = re.sub(r'\\s*\\.\\s*', '. ', description)              # clean periods\n",
    "        df.loc[name, 'description'] = description.strip().lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = remove_name_from_description(df)\n",
    "\n",
    "#remove abbreviation from description\n",
    "def remove_abbreviation(row):\n",
    "    words = row.name.split()\n",
    "    to_remove = ['de', 'des', 'le', 'la', 'les', 'du', 'd\\'', 'l\\'', 'un', 'une', 'et', '&']\n",
    "    for rem in to_remove: \n",
    "        if rem in words: words.remove(rem)\n",
    "        \n",
    "    abbr = \"\".join([word[0] for word in words])\n",
    "    abbr = re.escape(abbr)\n",
    "    pattern = rf\"\\(*\\s*{abbr}\\s*\\)*\"\n",
    "    return re.sub(pattern, \"\", str(row['description'])).strip()\n",
    "\n",
    "df['description'] = df.apply(remove_abbreviation, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a14686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_embedding column created with concatenated features for text embedding.\n"
     ]
    }
   ],
   "source": [
    "#generate missing description based on present \n",
    "\n",
    "def generate_description(row):\n",
    "    if row['description'] != 'unknown':\n",
    "        return row['description']\n",
    "    else: \n",
    "        return f\"a startup operating in {row['sector']} using {row['technologies']} based in {row['city']}\"\n",
    "\n",
    "df['generated_description'] = df.apply(generate_description, axis=1)\n",
    "\n",
    "#concatenate features for text embedding \n",
    "def concatenate_features(row):\n",
    "    features = []\n",
    "    if row['description'] != 'unknown':\n",
    "        features.append(f\"business description: {row['description']}\")\n",
    "        features.append(f\"industry sectors: {row['sector'].replace(';', ' ').replace(\"/\", ' ')}\")\n",
    "        features.append(f\"technologies used: {row['technologies'].replace(';', ' ').replace(\"/\", ' ')}\")\n",
    "        features.append(f\"located in: {row['city']}\")\n",
    "        return ' '.join(features)\n",
    "    \n",
    "    return row['generated_description'] #because the generated description is already a concatenation of features\n",
    "\n",
    "df['text_embedding'] = df.apply(concatenate_features, axis=1)\n",
    "\n",
    "print(\"text_embedding column created with concatenated features for text embedding.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c725d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\zakar\\OneDrive\\Bureau\\projects\\techatlas\\data\\embedding_data\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)  # make names a column again\n",
    "\n",
    "df.to_csv(PATHS[\"embedding_data\"], index=False, encoding='utf-8') #index=False to save names as a column\n",
    "print(\"Data saved to\", PATHS['embedding_data'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
